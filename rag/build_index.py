#!/usr/bin/env python3
"""
RAG ç´¢å¼•æ„å»ºå™¨ - æœè£…ä¾›åº”é“¾ç®¡ç†ç³»ç»Ÿ
================================
æ‰«æé¡¹ç›®æ–‡æ¡£å’Œå…³é”®ä»£ç ï¼Œåˆ‡å—ï¼Œä¿å­˜ä¸ºå¯æ£€ç´¢çš„ JSON ç´¢å¼•ã€‚

ç”¨æ³•ï¼š
    python3 rag/build_index.py
"""

import json
import os
import re
import sys
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
ROOT = Path(__file__).parent.parent

# -------------------------------------------------------------------
# è¦ç´¢å¼•çš„æ–‡ä»¶è§„åˆ™
# -------------------------------------------------------------------
DOC_FILES = [
    "å¼€å‘æŒ‡å—.md",
    "ç³»ç»ŸçŠ¶æ€.md",
    "ä¸šåŠ¡æµç¨‹è¯´æ˜.md",
    "è®¾è®¡ç³»ç»Ÿå®Œæ•´è§„èŒƒ-2026.md",
    "å¿«é€Ÿæµ‹è¯•æŒ‡å—.md",
    "INVENTORY_SYSTEM_GUIDE.md",
    "README.md",
    ".github/copilot-instructions.md",
]

DOC_DIRS = [
    "docs",
    "deployment",
]

CODE_GLOBS = [
    # åç«¯ç¼–æ’å™¨ï¼ˆä¸šåŠ¡æ ¸å¿ƒï¼‰
    ("backend/src/main/java", "**/*Orchestrator.java", "orchestrator"),
    # åç«¯Controllerï¼ˆAPIç«¯ç‚¹ï¼‰
    ("backend/src/main/java", "**/*Controller.java", "controller"),
    # å‰ç«¯æœåŠ¡å±‚
    ("frontend/src/services", "**/*.ts", "api"),
    # å‰ç«¯Store
    ("frontend/src/stores", "**/*.ts", "store"),
    # å‰ç«¯è·¯ç”±é…ç½®
    ("frontend/src", "routeConfig.ts", "config"),
    # å‰ç«¯å·¥å…·å‡½æ•°
    ("frontend/src/utils", "**/*.ts", "util"),
]

# ç´¢å¼•è¾“å‡ºè·¯å¾„
INDEX_FILE = ROOT / "rag" / "index.json"

# BM25 token æ–‡ä»¶
TOKENS_FILE = ROOT / "rag" / "tokens.json"

# -------------------------------------------------------------------
# ä¸­æ–‡+ä»£ç æ··åˆåˆ†è¯
# -------------------------------------------------------------------
def tokenize(text: str) -> list[str]:
    """ä½¿ç”¨ jieba åˆ†è¯ï¼Œå…¼å®¹ä¸­è‹±æ–‡ä»£ç æ··åˆå†…å®¹"""
    try:
        import jieba
        jieba.setLogLevel(60)  # é™é»˜
        # å¯¹ä»£ç æ ‡è¯†ç¬¦åšé¢å¤–åˆ†å‰²ï¼ˆcamelCase â†’ å°å†™è¯ï¼‰
        text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
        tokens = list(jieba.cut(text, cut_all=False))
        # è¿‡æ»¤å•å­—ç¬¦æ— æ„ä¹‰ tokenï¼ˆç©ºç™½ã€æ ‡ç‚¹ï¼‰
        return [t.strip().lower() for t in tokens if len(t.strip()) > 1]
    except ImportError:
        # fallbackï¼šæŒ‰æ±‰å­—è¾¹ç•Œ + è‹±æ–‡å•è¯åˆ†å‰²
        tokens = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z][a-zA-Z0-9_]*', text)
        return [t.lower() for t in tokens if len(t) > 1]


# -------------------------------------------------------------------
# æ–‡æ¡£åˆ‡å—ç­–ç•¥ï¼ˆæŒ‰ Markdown æ ‡é¢˜åˆ†æ®µï¼‰
# -------------------------------------------------------------------
def chunk_markdown(content: str, source: str, doc_type: str) -> list[dict]:
    """æŒ‰ ## æ ‡é¢˜åˆ‡å—ï¼Œè¶…é•¿æ®µè½å†ç­‰åˆ†ã€‚"""
    chunks = []
    MAX_CHARS = 800
    OVERLAP = 100

    # æŒ‰ ## æ ‡é¢˜åˆ†å‰²
    sections = re.split(r'\n(?=#{1,3} )', content)

    for section in sections:
        section = section.strip()
        if len(section) < 30:
            continue

        # æå–æ ‡é¢˜
        title_match = re.match(r'^(#{1,3})\s+(.+)', section)
        title = title_match.group(2).strip() if title_match else ""

        if len(section) <= MAX_CHARS:
            chunks.append({
                "source": source,
                "type": doc_type,
                "title": title,
                "content": section,
            })
        else:
            # è¶…é•¿æ®µè½ï¼šæ»‘åŠ¨çª—å£åˆ‡åˆ†
            words = section.split('\n')
            buf = []
            buf_len = 0
            for line in words:
                buf.append(line)
                buf_len += len(line)
                if buf_len >= MAX_CHARS:
                    text = '\n'.join(buf)
                    chunks.append({
                        "source": source,
                        "type": doc_type,
                        "title": title,
                        "content": text,
                    })
                    # overlapï¼šä¿ç•™æœ€åå‡ è¡Œ
                    overlap_lines = []
                    overlap_len = 0
                    for l in reversed(buf):
                        overlap_len += len(l)
                        overlap_lines.insert(0, l)
                        if overlap_len >= OVERLAP:
                            break
                    buf = overlap_lines
                    buf_len = overlap_len
            if buf:
                text = '\n'.join(buf).strip()
                if len(text) > 30:
                    chunks.append({
                        "source": source,
                        "type": doc_type,
                        "title": title,
                        "content": text,
                    })

    return chunks


def chunk_code(content: str, source: str, code_type: str) -> list[dict]:
    """ä»£ç æ–‡ä»¶ï¼šæŒ‰ç±»/æ–¹æ³•åˆ‡å—ï¼Œè¶…é•¿ç­‰åˆ†ã€‚"""
    chunks = []
    MAX_CHARS = 600

    lines = content.split('\n')
    buf = []
    buf_len = 0
    current_title = Path(source).name

    for line in lines:
        # è·³è¿‡çº¯æ³¨é‡Šå—ï¼ˆä¿ç•™ Javadocï¼‰
        stripped = line.strip()
        if stripped.startswith('//') and not stripped.startswith('///'):
            continue

        # è¯†åˆ«ç±»/æ–¹æ³•è¾¹ç•Œä½œä¸º chunk æ ‡é¢˜
        java_class = re.match(r'\s*(?:public|private|protected)?\s*(?:class|interface|enum)\s+(\w+)', line)
        java_method = re.match(r'\s*(?:public|private|protected|static|final|\s)+\s+\w+\s+(\w+)\s*\(', line)
        ts_fn = re.match(r'\s*(?:export\s+)?(?:const|function|async function)\s+(\w+)', line)

        if java_class:
            current_title = f"class {java_class.group(1)}"
        elif java_method and buf_len > 50:
            current_title = f"method {java_method.group(1)}"
        elif ts_fn:
            current_title = f"fn {ts_fn.group(1)}"

        buf.append(line)
        buf_len += len(line)

        if buf_len >= MAX_CHARS:
            text = '\n'.join(buf).strip()
            if len(text) > 50:
                chunks.append({
                    "source": source,
                    "type": code_type,
                    "title": current_title,
                    "content": text,
                })
            buf = buf[-10:]  # overlap
            buf_len = sum(len(l) for l in buf)

    if buf:
        text = '\n'.join(buf).strip()
        if len(text) > 50:
            chunks.append({
                "source": source,
                "type": code_type,
                "title": current_title,
                "content": text,
            })

    return chunks


# -------------------------------------------------------------------
# ä¸»æµç¨‹
# -------------------------------------------------------------------
def main():
    chunks = []

    # 1. å›ºå®šæ–‡æ¡£æ–‡ä»¶
    for rel_path in DOC_FILES:
        path = ROOT / rel_path
        if not path.exists():
            print(f"  [skip] {rel_path} (ä¸å­˜åœ¨)")
            continue
        content = path.read_text(encoding="utf-8", errors="ignore")
        source = rel_path
        new_chunks = chunk_markdown(content, source, "doc")
        chunks.extend(new_chunks)
        print(f"  [doc] {rel_path} â†’ {len(new_chunks)} chunks")

    # 2. æ–‡æ¡£ç›®å½•
    for dir_rel in DOC_DIRS:
        dir_path = ROOT / dir_rel
        if not dir_path.exists():
            continue
        for md_file in sorted(dir_path.glob("**/*.md")):
            rel = str(md_file.relative_to(ROOT))
            content = md_file.read_text(encoding="utf-8", errors="ignore")
            new_chunks = chunk_markdown(content, rel, "doc")
            chunks.extend(new_chunks)
            print(f"  [doc] {rel} â†’ {len(new_chunks)} chunks")

    # 3. ä»£ç æ–‡ä»¶
    for base_rel, pattern, code_type in CODE_GLOBS:
        base = ROOT / base_rel
        if not base.exists():
            continue
        for code_file in sorted(base.glob(pattern)):
            # è·³è¿‡æµ‹è¯•æ–‡ä»¶
            if "test" in str(code_file).lower() or "Test" in code_file.name:
                continue
            rel = str(code_file.relative_to(ROOT))
            try:
                content = code_file.read_text(encoding="utf-8", errors="ignore")
            except Exception:
                continue
            if len(content) < 100:
                continue
            new_chunks = chunk_code(content, rel, code_type)
            chunks.extend(new_chunks)
            print(f"  [code/{code_type}] {code_file.name} â†’ {len(new_chunks)} chunks")

    # 4. ç»™æ¯ä¸ª chunk åŠ  id
    for i, chunk in enumerate(chunks):
        chunk["id"] = i

    # 5. ä¿å­˜ chunks
    INDEX_FILE.write_text(json.dumps(chunks, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"\nâœ… ç´¢å¼•å®Œæˆï¼š{len(chunks)} ä¸ª chunks â†’ {INDEX_FILE}")

    # 6. é¢„è®¡ç®— BM25 tokens
    print("â³ åˆ†è¯ä¸­ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼‰...")
    token_list = [tokenize(c["title"] + " " + c["content"]) for c in chunks]
    TOKENS_FILE.write_text(json.dumps(token_list, ensure_ascii=False), encoding="utf-8")
    print(f"âœ… Token ç´¢å¼•å®Œæˆ â†’ {TOKENS_FILE}")


if __name__ == "__main__":
    print(f"ğŸ” å¼€å§‹æ„å»ºç´¢å¼•ï¼Œé¡¹ç›®æ ¹: {ROOT}\n")
    main()
